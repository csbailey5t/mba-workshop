---
title: "Exploring Data with R"
author: "Scott Bailey"
date: "5/1/2020"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Exploring Data with R

May 5, 2020

Instructors:
  - Scott Bailey
  - Natalia Lopez 
  
## Approach and Learning Goals

We're going to take a very pragmatic approach in our workshop today. Instead of thinking about R as a programming language we're trying to learn to code in, we're going to approach R as just a tool that we can use to do some fundamental data exploration and analysis. In that line of thinking, instead of learning a lot of base R, and instead of building up an understanding of syntax and semantics according to fundamental coding concepts, we're going to start using the tidyverse packages and approach right away in an applied manner. Briefly, tidyverse is a set of opinionated packages with a coherent approach to how to work with data in R. For many of us, they make R easier to learn and to use.

We'll use tidyverse to do a few specific things today, which will give us a general data exploration and analysis workflow:

1. Import data
2. Get summary statistics on data
3. Pre-process/clean data
4. Visualize data
5. Fit a model to some of our data

Our goal today isn't to get through every single line of code or every idea. Most importantly, I want you to have the experience of writing code with data and making things happen. 

## Orientation to RStudio

RStudio is a full-featured integrated development environment for R. Let's take a brief tour through RStudio's interface, including a look at the `Preferences`. 

## Creating a new R script

There are several different formats of R scripts, including RMarkdown files and R Notebooks. Just to stay focused on learning and writing R code today, we're going to stick with the straightforward `.R` scripts. 

* Create new `.R` script and name it `exploration.R`. 

## Installing and importing packages

By default, you will have base R and and all it's capacity in your script. In order to use the tidyverse packages, and any other of the many great R packages, you'll need to install them for your version of R, and then import them into your script. 

```{r install_packages, results='hide', eval=FALSE}
install.packages(c("tidyverse",
                   "forcats",
                   "lubridate",
                   "skimr",
                   "mgcv",
                   "gratia"))
```

```{r import_libraries, results='hide'}
library(tidyverse, warn.conflicts = FALSE)
library(forcats)
library(lubridate)
library(skimr)
library(mgcv)
library(gratia)
```

## Importing Data

The data you provided for this workshop is anonymized sales related data for a major big-box store. There are a number of files, documented in the Readme file I sent out along with the data. We're going to focus on two particular files, and import both right here at the beginning. 

```{r import_data}
features_df <- read_csv("data/features.csv")
train_df <- read_csv("data/train.csv")
```

Once you've run these lines of code, you should see new data shown in the `Environment` pane, along with the dimensions of the data. We can open up this data and view it as a table, but let's hold off on doing that and instead explore it a bit with R code. 

## Getting an overview of our data

There's a few different ways that we can get a high-level sense of our data. We'll start with the `glimpse` function from the tidyverse and start wth the data stored in `features_df`.  

```{r}
glimpse(features_df)
```

We're able to see how many rows we have, how many columns or variables, their data types, and the first values in those columns. The data being shown organized by columns is a hint here; R works most effectively when you think about operating on columns of data. 

If we want to take a bit deeper look while still staying at a high level, we can use the `summary` function from base R. 

```{r}
summary(features_df)
```

This isn't bad - we're getting quartile breakdowns of numeric variables now. We can do a bit better with the `skim` function from the `skimr` package though, and it's my go to when I just need to dig into some data. 

```{r}
skim(features_df)
```

What do get here that we didn't before?

Let's pay attention to the types of our variables. R does it's best to figure out the data types, but I notice one that seems not quite right to me: Store. Right now it's a numeric, but each of these numbers is really a unique identifier for a store. As such, we can consider them categorical data. Let's go ahead and convert that column to the factor data type. 

```{r}
features_df$Store <- as_factor(features_df$Store)

skim(features_df$Store)
```

We had been using these overview functions - `summary`, `glimpse`, and `skim` - on the whole dataframe or tibble, but we can also check out single columns. 

## Subsetting data

What if we want to narrow down our data of interest rather than looking at the whole 8000+ observations? The tidyverse provides a set of functions for common subsetting tasks. We'll start with `filter`.

Let's get back just the data for store 1. 

```{r}
filter(features_df, Store == 1)
```

We haven't stored this subset anywhere yet, just printed it out to the standard output. We'll get to saving the data into a new variable shortly. 

The `filter` function takes at least two arguments: the dataframe you want to filter, and a condition to determine what to include. Here's our condition is where the value in the `Store` variable is 1. 

There's a more common way to use these "verbs" in tidyverse, though, that let's us chain together multiple actions. In the spirit of Unix, we have `pipes`, unidirectional data flows, which use this character: `%>%`. Let's see it in action. 
```{r}
features_df %>%
  filter(Store == 1)
```

The two code blocks above are the same, but you'll more often see the second version. When we use a pipe, we send the data in the variable in front of the pipe to the function after the pipe. We pipe the data into the function. This data goes into the function as the first parameter, and R knows that what we then put into the function are the rest of the parameters, such as the condition. 

If that doesn't make sense yet, don't worry. We're going to keep using it, and we'll build up our knowledge with practice. 

Let's run that code again, but save the filtered dataset we get back this time into a new variable, `features_store_1`. 

```{r}
features_store_1 <- features_df %>%
  filter(Store == 1)

skim(features_store_1)
```

We can combine filter conditions by chaining together filter statements or just adding multiple conditions to a single filter function. 

```{r}
features_df %>%
  filter(Store == 1 & Date > as_date("2012-01-01"))

# is the same as

features_df %>%
  filter(Store == 1) %>%
  filter(Date > as_date("2012-01-01"))

# is the same as 

features_store_1 %>%
  filter(Date > as_date("2012-01-01"))

```

We can also subset by columns with the `select` verb or function. We'll start with our full `features_df` data, filter it by store, then select just some columns. 

```{r}
features_df %>%
  filter(Store <= 10) %>%
  select(Store, Date, Temperature, Fuel_Price)
```

Our code doesn't work. Any ideas why?

```{r}
features_df %>%
  filter(Store %in% c(1,10)) %>%
  select(Store, Date, Temperature, Fuel_Price)
```

What did we do differently that made our code work?

`filter` and `select` are very powerful, and we've just scratched the surface. We're going to keep moving, though, so we can try to get through a more full workflow. 

## Pre-processing or cleaning data

There are a few different things we commonly think about in pre-proccessing and cleaning data. First, we might think about missing data. Let's check how much missing data we have. 

```{r}
features_df %>%
  skim()
```

The various `Markdown` columns have quite a bit of missing data, while CPI and Unemployment have some. There are a lot of strategies for handling missing data. Since we're just doing a quick tour through R and tidyverse, right now we're just going to drop the rows with missing values in `CPI` and `Unemployment` while getting rid of the `Markdown` columns completely. 

```{r}
features_df_clean <- features_df %>%
  drop_na(CPI, Unemployment) %>%
  select(-starts_with("Markdown"))
```

During pre-processing, we may also create derivative columns that we might need for our analysis. In this case, let's break the `Date` column up, and create columns for `Month`, `Day`, `Week`, and `Year`. You could treat the dates as strings, and split the date on the hyphen character. It's better to take advantage of R knowing that the `Date` column has data of a `Date` type though. We'll use functions from the great `lubridate` library to create these columns. 

```{r}
features_df_clean <- features_df_clean %>%
  mutate(
    Day = day(Date),
    Month = month(Date),
    Year = year(Date),
    Week = week(Date),
  )

head(features_df_clean)
```

That's all we're going to do for now with pre-processing, but it's good to think about how flexible this pattern is with `mutate`. 

## Visualizing Data

We've been looking at our data in tables so far, but let's switch to using visualization to get some sense of the data. The most popular plotting tool in R these days is `ggplot2`, which is part of the tidyverse. It is structured by the idea of the [Grammar of Graphics](https://vita.had.co.nz/papers/layered-grammar.html), and allows us to build up visualizations layer by layer. 

Let's start with a quick scatter plot. Notice the switch from the pipe (`%>%`) to the plus sign in the `ggplot` code.

```{r}
features_df %>%
  ggplot(aes(x = Date, y = Temperature)) +
  geom_point()
```

Since we're mapping columns to different aspects of the graphic, let's try to add in a bit more data. 

```{r}
features_df %>%
  ggplot(aes(x = Date, y = Temperature)) +
  geom_point(aes(color = Store))
```

This is still really busy. Let's take a sample of the data, graph that, and then place around with the opacity of the points to try to improve the image. 

```{r}
sample_stores = sample(1:50, 10)
features_df %>%
  filter(Store %in% sample_stores) %>%
  ggplot(aes(x = Date, y = Temperature)) +
  geom_point(aes(color=Store), alpha=0.4)
```

We can start to see trends in the temperature here, differentiated by individual stores. 

We can use other shapes to get a sense of the distribution of our data. Let's build a histogram that let's us explore the distribution of the CPI over the whole dataset. 

```{r}
features_df %>%
  ggplot(aes(CPI)) +
  geom_histogram(fill = "white", color = "grey30")
```

This shape is interesting. I'm definitely not a business person or an economist, so there might be a clear explanation you know for why we have such a high number of observations with a low CPI, then a huge gap in the middle. 

Let's add some reference lines to this graph: a mean line that will help us see how much of this distribution falls above or below the mean CPI for the data, and the same for median. 

```{r}
features_df %>%
  ggplot(aes(CPI)) +
  geom_histogram(fill = "white", color = "grey30") +
  geom_vline(aes(xintercept = mean(features_df$CPI, na.rm = TRUE)), color = "red") +
  geom_vline(aes(xintercept = median(features_df$CPI, na.rm = TRUE)), color = "blue")
```


We're going to keep playing around with `ggplot2`, but let's also learn a bit about aggregate measures while we're doing that.

## Aggregate measures

The tidyverse way to do aggregations is using the "split-apply-combine" approach, facilitated by functions like `group_by` and `summarise`. Let's see it in action. 

Maybe we want to know what the average temperature is for each store. We can group the observations by `Store`, and take the mean of the values in the `Temperature` column. 

```{r}
features_df %>%
  group_by(Store) %>%
  summarise(avg_temp = mean(Temperature))
```

We can sort this data by the value in `avg_temp`.

```{r}
features_df %>%
  group_by(Store) %>%
  summarise(avg_temp = mean(Temperature)) %>%
  arrange(desc(avg_temp))
```


The result itself can be saved as a dataframe or tibble, then modified or used as we want. We can also just add `ggplot` code to the end of our statement to see results on the fly when we're just doing some data exploration. 

```{r}
features_df %>%
  group_by(Store) %>%
  summarise(avg_temp = mean(Temperature)) %>%
  ggplot(aes(x = Store, y = avg_temp)) +
  geom_point()
```

We've just used `mean` throughout this example, but you could use any number of different built in or custom functions, as long as they take a vector or series of values, and condense them to a single value. 

## A brief look at modeling

Let's switch over to our other dataset, stored in `train_df`, and apply some of what we've learned. At the end, we'll also take a quick look at modeling. 

We'll start with a high-level look at the data. 

```{r}
skim(train_df)
```
 
Here each row or observation is the sales data for a week for a single department in a single store. There are 421,570 observations, so we may not have much success trying to put every single point on a graph. Let's make the `Store` and `Dept` columns factors, then start with some filtering. 

```{r}
train_df$Store = as_factor(train_df$Store)
train_df$Dept = as_factor(train_df$Dept)


train_df %>%
  filter(Store == 1) %>%
  filter(Dept == 1) %>%
  ggplot(aes(Date, Weekly_Sales, color = IsHoliday)) +
  geom_point()
```

We can see that there is some pattern to the sales of a single department in a single store, though it's a bit loose at points. Let's try another department. 

```{r}
train_df %>%
  filter(Store == 1) %>%
  filter(Dept == 2) %>%
  ggplot(aes(Date, Weekly_Sales, color = IsHoliday)) +
  geom_point()
```

This looks like a pretty different pattern. We could definitely keep going like this, and could programmatically generate and save figures like this for every department and every store. Let's do a bit of aggregation, though, to look at the whole data set. 

Maybe we're interested in the total sales for each store over the multi-year period. 

```{r}
train_df %>%
  group_by(Store) %>%
  summarise(total_sales = sum(Weekly_Sales)) %>%
  ggplot(aes(x = Store, y = total_sales)) +
  geom_col()
```

Some of these stores have had substantially higher sales than others. 

We could also group the data by department instead, to see which departments typically sell more. 

```{r}
train_df %>%
  group_by(Dept) %>%
  summarise(total_sales = sum(Weekly_Sales)) %>%
  ggplot(aes(x = Dept, y = total_sales)) +
  geom_col()
```


As a final exercise, let's think about how we might model some of this data, and see what R could do for us there. We will need to create a column for the `Week` as we did in the features data. 

```{r}
train_mod <- train_df %>%
  filter(Store == 1) %>%
  filter(Dept == 1) %>%
  mutate(
    Day = day(Date),
    Month = month(Date),
    Year = year(Date),
    Week = week(Date)
  )

train_mod %>%
  ggplot(aes(x = Week, y = Weekly_Sales)) +
  geom_point()
```


We have a non-linear shape with multiple curves, so we'll try out a generalized additive model.

```{r}
gam_mod <- gam(Weekly_Sales ~ s(Week), data = train_mod, method="REML")

draw(gam_mod) 
plot(gam_mod, residuals = TRUE, pch = 1, cex = 1, shade = TRUE)
```

This is just a first pass at a model. We could pull the coefficients and examine them. We could start modifying parameters in the gam, and try to pick up a bit more on the two high curves in the early distribution of the data. For now, though, it's enough to see that we can do this advanced sort of analysis fairly quickly because of the great packages that have been created for R. For anything related to data exploration, analysis, visualization, R is incredibly power, and incredibly flexible. 

## Further Resources

There is a ton of great content to help with learning R online. Here's just a few resources that I think are solid. 

- Hadley Wickham's R for Data Science - the essential guide from the creator of the tidyverse: https://r4ds.had.co.nz/
- DataQuest - online learning - free and paid: https://www.dataquest.io/path/data-analyst-r/ 
- Chris Engelhardt's list of data science resources: https://github.com/Chris-Engelhardt/data_sci_guide
- Roger Peng's R Programming for Data Science: https://bookdown.org/rdpeng/rprogdatascience/
- Jenny Bryan's Stats545 course: https://stat545.com/
- RStudio Conf videos - more advanced sometimes, but great: https://rstudio.com/resources/rstudioconf-2020/
- For intermediate folks with a bit of experience already, What They Forgot to Teach You about R from Jenny Bryan and Jim Hester is wonderful, and should be definitive for best practices for projects and workflows: https://rstats.wtf/
